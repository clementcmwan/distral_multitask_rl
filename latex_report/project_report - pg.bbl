\begin{thebibliography}{10}

\bibitem{spinningupRL}
Josh Achiam et~al.
\newblock Spinning up in deep rl, 2018--.
\newblock [Online; accessed 9-8-2019].

\bibitem{fox2015taming}
Roy Fox, Ari Pakman, and Naftali Tishby.
\newblock Taming the noise in reinforcement learning via soft updates.
\newblock {\em arXiv preprint arXiv:1512.08562}, 2015.

\bibitem{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock {\em arXiv preprint arXiv:1801.01290}, 2018.

\bibitem{haarnoja2018soft1}
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha,
  Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock {\em arXiv preprint arXiv:1812.05905}, 2018.

\bibitem{hessel2018rainbow}
Matteo Hessel, Joseph Modayil, Hado Van~Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In {\em Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem{hessel2019multi}
Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt,
  and Hado van Hasselt.
\newblock Multi-task deep reinforcement learning with popart.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3796--3803, 2019.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{jaderberg2016reinforcement}
Max Jaderberg, Volodymyr Mnih, Wojciech~Marian Czarnecki, Tom Schaul, Joel~Z
  Leibo, David Silver, and Koray Kavukcuoglu.
\newblock Reinforcement learning with unsupervised auxiliary tasks.
\newblock {\em arXiv preprint arXiv:1611.05397}, 2016.

\bibitem{lample2017playing}
Guillaume Lample and Devendra~Singh Chaplot.
\newblock Playing fps games with deep reinforcement learning.
\newblock In {\em Thirty-First AAAI Conference on Artificial Intelligence},
  2017.

\bibitem{mackay2003information}
David~JC MacKay and David~JC Mac~Kay.
\newblock {\em Information theory, inference and learning algorithms}.
\newblock Cambridge university press, 2003.

\bibitem{mitchell1993explanation}
Tom~M Mitchell and Sebastian~B Thrun.
\newblock Explanation-based neural network learning for robot control.
\newblock In {\em Advances in neural information processing systems}, pages
  287--294, 1993.

\bibitem{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  1928--1937, 2016.

\bibitem{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1312.5602}, 2013.

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529, 2015.

\bibitem{nachum2017bridging}
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2775--2785, 2017.

\bibitem{parisotto2015actor}
Emilio Parisotto, Jimmy~Lei Ba, and Ruslan Salakhutdinov.
\newblock Actor-mimic: Deep multitask and transfer reinforcement learning.
\newblock {\em arXiv preprint arXiv:1511.06342}, 2015.

\bibitem{rawlik2013stochastic}
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar.
\newblock On stochastic optimal control and reinforcement learning by
  approximate inference.
\newblock In {\em Twenty-Third International Joint Conference on Artificial
  Intelligence}, 2013.

\bibitem{rusu2015policy}
Andrei~A Rusu, Sergio~Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins,
  James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and
  Raia Hadsell.
\newblock Policy distillation.
\newblock {\em arXiv preprint arXiv:1511.06295}, 2015.

\bibitem{rusu2016progressive}
Andrei~A Rusu, Neil~C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.
\newblock {\em arXiv preprint arXiv:1606.04671}, 2016.

\bibitem{schaul2015universal}
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver.
\newblock Universal value function approximators.
\newblock In {\em International Conference on Machine Learning}, pages
  1312--1320, 2015.

\bibitem{schmitt2018kickstarting}
Simon Schmitt, Jonathan~J Hudson, Augustin Zidek, Simon Osindero, Carl Doersch,
  Wojciech~M Czarnecki, Joel~Z Leibo, Heinrich Kuttler, Andrew Zisserman, Karen
  Simonyan, et~al.
\newblock Kickstarting deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1803.03835}, 2018.

\bibitem{schulman2017equivalence}
John Schulman, Xi~Chen, and Pieter Abbeel.
\newblock Equivalence between policy gradients and soft q-learning.
\newblock {\em arXiv preprint arXiv:1704.06440}, 2017.

\bibitem{schulman2015high}
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter
  Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock {\em arXiv preprint arXiv:1506.02438}, 2015.

\bibitem{shani2005resolving}
Guy Shani and Ronen~I Brafman.
\newblock Resolving perceptual aliasing in the presence of noisy sensors.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1249--1256, 2005.

\bibitem{sharma2017learning}
Sahil Sharma, Ashutosh Jha, Parikshit Hegde, and Balaraman Ravindran.
\newblock Learning to multi-task by active sampling.
\newblock {\em arXiv preprint arXiv:1702.06053}, 2017.

\bibitem{sharma2017online}
Sahil Sharma and Balaraman Ravindran.
\newblock Online multi-task learning using active sampling.
\newblock {\em arXiv preprint arXiv:1702.06053}, 2017.

\bibitem{silver2015}
David Silver.
\newblock Lecture notes in reinforcement learning, 2015.

\bibitem{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{sutton2000policy}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Advances in neural information processing systems}, pages
  1057--1063, 2000.

\bibitem{sutton2011horde}
Richard~S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick~M
  Pilarski, Adam White, and Doina Precup.
\newblock Horde: A scalable real-time architecture for learning knowledge from
  unsupervised sensorimotor interaction.
\newblock In {\em The 10th International Conference on Autonomous Agents and
  Multiagent Systems-Volume 2}, pages 761--768. International Foundation for
  Autonomous Agents and Multiagent Systems, 2011.

\bibitem{sutton1999between}
Richard~S Sutton, Doina Precup, and Satinder Singh.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock {\em Artificial intelligence}, 112(1-2):181--211, 1999.

\bibitem{teh2017distral}
Yee Teh, Victor Bapst, Wojciech~M Czarnecki, John Quan, James Kirkpatrick, Raia
  Hadsell, Nicolas Heess, and Razvan Pascanu.
\newblock Distral: Robust multitask reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4496--4506, 2017.

\bibitem{thomas2017policy}
Philip~S Thomas and Emma Brunskill.
\newblock Policy gradient methods for reinforcement learning with function
  approximation and action-dependent baselines.
\newblock {\em arXiv preprint arXiv:1706.06643}, 2017.

\bibitem{watkins1992q}
Christopher~JCH Watkins and Peter Dayan.
\newblock Q-learning.
\newblock {\em Machine learning}, 8(3-4):279--292, 1992.

\bibitem{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning}, 8(3-4):229--256, 1992.

\bibitem{zhang2019pretrain}
Xiaoqin Zhang, Yunfei Li, Huimin Ma, and Xiong Luo.
\newblock Pretrain soft q-learning with imperfect demonstrations.
\newblock {\em arXiv preprint arXiv:1905.03501}, 2019.

\end{thebibliography}
